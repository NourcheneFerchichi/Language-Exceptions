{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SightengineNLP-TechnicalAssignment-NourcheneFerchichi.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1.\n",
        "What approach would you use to automatically determine what languages are used in a given message? Please keep in mind that messages do not always contain two different languages, they can also contain just one language. If you believe that some situations cannot be handled by your implementation, please specify which ones and why."
      ],
      "metadata": {
        "id": "9S5m4deiQ3Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer to Question 1.\n",
        "The first idea was to tokenize the entire message into words and check the language for each word. But this method showed poor results as two different languages may share similar words.\n",
        "\n",
        "\n",
        "Instead, I used N-grams (a sequence of successive N words) to detect the existent languages in the user's message. N-grams are used for a variety of things: auto completion of sentences, auto spell check... Here, I would like to check if using N-grams for languages detection (in a multilingual environment) would give good results. \n",
        "\n",
        "\n",
        "* I started by generating all the N-grams (where N is a parameter to be optimized: 3-gram for example) for a given message. Then, I used `langdetect`, a pre-built package for automatic language detection applied to each N-gram sequence separately. The good thing about `langdetect`, is that it is well optimized and that it supports 55 languages including French, English, Arabic, Chinese ...\n",
        "\n",
        "\n",
        "* I then calculated the frequency of each detected language. If the first most frequent language is predominant (with a frequency of 0.80), I only output this language. If not, I extract the top `max_lang` most frequent (Where `max_lang` is a user parameter: top 2 for example)\n",
        "\n",
        "* The solution is then tested using different messages with multiple languages.\n",
        "\n",
        "The remark is that the solution is sensitive to the choice of `words_to_combine`, `max_lang`. So those parameters need to be tuned correctly. Adding to that, the Language detection algorithm is non-deterministic, which means that if you try to run it on a text which is either too short or too ambiguous, you might get different results every time you run it.\n",
        "\n",
        "Further improvement could be by training your own Neural Network language detection model.\n",
        "\n",
        "Have a look at the python implementation below!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FQJUSyhxU1gC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xrGQ3E_Qsxr",
        "outputId": "b1d0ef77-ba23-442d-836c-7295a659b665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from langdetect import detect\n",
        "from typing import List\n",
        "\n",
        "class Solution():\n",
        "  \"\"\"\n",
        "  Detect languages in a text message, in a context where users often write \n",
        "  messages containing multiple languages.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, words_to_combine: int, message: str, max_lang: int):\n",
        "    \"\"\" Class initialiser\n",
        "    \n",
        "    Args:\n",
        "        words_to_combine (int): The number N of N-grams.\n",
        "        message (str): The user's multilangual text message.\n",
        "        max_lang (int): The maximun number of languages to be detected.\n",
        "    \"\"\"\n",
        "    self.words_to_combine = words_to_combine\n",
        "    self.message = message\n",
        "    self.max_lang = max_lang\n",
        "\n",
        "  def generate_ngrams(self) -> List[str]:\n",
        "      \"\"\" Creates an N-grams of all possible combinations of “N” successive\n",
        "      words from a text.\n",
        "      \n",
        "      Returns:\n",
        "          output (List[str]): The list of the generated N-grams. \n",
        "      \"\"\"\n",
        "      words = self.message.split()\n",
        "      output = []  \n",
        "      for i in range(len(words)-self.words_to_combine+1):\n",
        "          output.append(\" \".join(words[i:i+self.words_to_combine-1]))\n",
        "      return output\n",
        "\n",
        "  def detect_languages(self):\n",
        "    \"\"\"Detect the language of a text message. \n",
        "    \n",
        "    Returns:\n",
        "        detected_languages (dict): The dictionary of detected languages.\n",
        "                detected_languages[n_gram_text] = language\n",
        "        n (int): The number on generated N-grams.\n",
        "    \"\"\"\n",
        "    detected_languages = {}\n",
        "    n_grams = self.generate_ngrams()\n",
        "    for sub_set in n_grams:\n",
        "        detected_languages[sub_set] = detect(sub_set)\n",
        "    return detected_languages, len(n_grams)\n",
        "    \n",
        "  def get_top_languages(self) -> None:\n",
        "      \"\"\"Extract the top self.max_lang languages\n",
        "      \"\"\"\n",
        "      detected_languages, total_grams = self.detect_languages()\n",
        "      count = Counter(detected_languages.values())\n",
        "      sorted_count = sorted(\n",
        "          dict(count).items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      if sorted_count[0][1]/total_grams >= 0.80: # Only one Language is present\n",
        "        detected_languages = sorted_count[0]\n",
        "        print(f\"The detected language in: '%s' is: %s\" %(self.message, detected_languages[0][0].upper()))\n",
        "      else:\n",
        "        detected_languages = sorted_count[:self.max_lang] # Multiple languages are present\n",
        "        print(f\"The detected languages in: '%s' are: %s and %s\" %(self.message, detected_languages[0][0].upper(), detected_languages[1][0].upper()))\n",
        "\n",
        "### Test Solution ###\n",
        "document = [\"Hello, tu as vu Lost in the Middle of Night l’autre jour ?\",\n",
        "        \"This is an English sentence written in english, dans un endroit frais et sec\",\n",
        "        \"Who are you? 小家伙 or just what we call 非常小的家伙\",\n",
        "        \"توفر Analytics Vidhya بوابة معرفية قائمة على المجتمع لمحترفي التحليلات وعلوم البيانات\",\n",
        "        \"Hello, how are you doing?\"]\n",
        "\n",
        "solution = Solution(words_to_combine=3, message=document[1], max_lang=2)\n",
        "solution.get_top_languages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhcCslCmQ6Xp",
        "outputId": "6e7cb63b-af79-4819-9fc1-9c201140c9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The detected languages in: 'This is an English sentence written in english, dans un endroit frais et sec' are: EN and FR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2.\n",
        "How would you proceed to detect if a given word is a variation of the word “blackhat”? Please write a Python function that determines if a string is a variation of this word. How would you generalize this to any given term?"
      ],
      "metadata": {
        "id": "BGBnJePMQ-1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer to Question 2.\n",
        "Based on the given example: *blackkkhat*, *bl@khat*, *b__la-c_k_hat* are variations of the word *blackhat*. Those variations include non alpha characters and duplicated ones, also the @ symbol was alternated with the a character.\n",
        "\n",
        "So the idea is to create a function that:\n",
        "* Replaces the \"@\" with \"a\"\n",
        "* Removes the duplicates\n",
        "* Removes all non alpha characters\n",
        "\n",
        "\n",
        "A generalisation on this would be by:\n",
        "* Removes the duplicates\n",
        "* Removes all non alpha characters\n",
        "* Creating a dictionary of the possible character to be alternated with other ones.\n",
        "\n",
        "Have a look at the python implementation below!"
      ],
      "metadata": {
        "id": "oKYwAb4pefjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(word: str) -> str:\n",
        "    \"\"\" Delete duplicates characters from a string\n",
        "        \n",
        "    Args:\n",
        "        word (str): A word with duplicated characters.\n",
        "\n",
        "    Returns:\n",
        "      Output (str): The word after deleting its duplicated characters.\n",
        "    \"\"\"\n",
        "    chars = []\n",
        "    prev = \"\"\n",
        "    for char in word:\n",
        "        if prev != char:\n",
        "            chars.append(char)\n",
        "            prev = char\n",
        "    output = ''.join(chars)\n",
        "    return output\n",
        "\n",
        "def check_obfuscation(original_word: str, variation_word: str) -> bool:\n",
        "    \"\"\" Verifies if an is variation_word the variation of the original_word\n",
        "\n",
        "    Args:\n",
        "        original_word (str): An original word.\n",
        "        variation_word (str): A possible obfuscation of the original one.\n",
        "\n",
        "    Returns:\n",
        "      same_word (bool): True if original_word and variation_word are similar,\n",
        "      False else.\n",
        "    \"\"\"\n",
        "    same_word = False\n",
        "    variation_word = variation_word.replace(\"@\", \"a\")\n",
        "    new_word = \"\".join(char for char in variation_word if char.isalpha())\n",
        "    new_word = remove_duplicates(new_word)\n",
        "    if original_word == new_word:\n",
        "        same_word = True\n",
        "    return same_word\n",
        "\n",
        "\n",
        "### Test Solution ###\n",
        "words = [\"blackkkhat\", \"bl@khat\", \"b__la-c_k_hat\", \"abcd\"]\n",
        "check_obfuscation(original_word=\"blackhat\", variation_word=words[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6raSogsQ_x8",
        "outputId": "bfa44a39-7b5d-4d4e-aa2d-5c4bcd1782cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NiEEYVO0RF4E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
